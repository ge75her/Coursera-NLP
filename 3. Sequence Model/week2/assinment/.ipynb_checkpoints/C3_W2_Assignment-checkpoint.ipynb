{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Jv7Y4hXwt0j"
   },
   "source": [
    "# Assignment 2:  Deep N-grams\n",
    "\n",
    "Welcome to the second assignment of course 3. In this assignment you will explore Recurrent Neural Networks `RNN`.\n",
    "- You will be using the fundamentals of google's [trax](https://github.com/google/trax) package to implement any kind of deeplearning model. \n",
    "\n",
    "By completing this assignment, you will learn how to implement models from scratch:\n",
    "- How to convert a line of text into a tensor\n",
    "- Create an iterator to feed data to the model\n",
    "- Define a GRU model using `trax`\n",
    "- Train the model using `trax`\n",
    "- Compute the accuracy of your model using the perplexity\n",
    "- Predict using your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8_3fIOdkGv1"
   },
   "source": [
    "## Outline\n",
    "\n",
    "- [Overview](#0)\n",
    "- [Part 1: Importing the Data](#1)\n",
    "    - [1.1 Loading in the data](#1.1)\n",
    "    - [1.2 Convert a line to tensor](#1.2)\n",
    "        - [Exercise 01](#ex01)\n",
    "    - [1.3 Batch generator](#1.3)\n",
    "        - [Exercise 02](#ex02)\n",
    "    - [1.4 Repeating Batch generator](#1.4)        \n",
    "- [Part 2: Defining the GRU model](#2)\n",
    "    - [Exercise 03](#ex03)\n",
    "- [Part 3: Training](#3)\n",
    "    - [3.1 Training the Model](#3.1)\n",
    "        - [Exercise 04](#ex04)\n",
    "- [Part 4:  Evaluation](#4)\n",
    "    - [4.1 Evaluating using the deep nets](#4.1)\n",
    "        - [Exercise 05](#ex05)\n",
    "- [Part 5: Generating the language with your own model](#5)    \n",
    "- [Summary](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JP0inrk5kGv3"
   },
   "source": [
    "<a name='0'></a>\n",
    "### Overview\n",
    "\n",
    "Your task will be to predict the next set of characters using the previous characters. \n",
    "- Although this task sounds simple, it is pretty useful.\n",
    "- You will start by converting a line of text into a tensor\n",
    "- Then you will create a generator to feed data into the model\n",
    "- You will train a neural network in order to predict the new set of characters of defined length. \n",
    "- You will use embeddings for each character and feed them as inputs to your model. \n",
    "    - Many natural language tasks rely on using embeddings for predictions. \n",
    "- Your model will convert each character to its embedding, run the embeddings through a Gated Recurrent Unit `GRU`, and run it through a linear layer to predict the next set of characters.\n",
    "\n",
    "<img src = \"images/model.png\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "The figure above gives you a summary of what you are about to implement. \n",
    "- You will get the embeddings;\n",
    "- Stack the embeddings on top of each other;\n",
    "- Run them through two layers with a relu activation in the middle;\n",
    "- Finally, you will compute the softmax. \n",
    "\n",
    "To predict the next character:\n",
    "- Use the softmax output and identify the word with the highest probability.\n",
    "- The word with the highest probability is the prediction for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RVSwzQ5Bwt0m",
    "outputId": "9b51a13e-cf54-457f-e1ea-2574f9d67453"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "\n",
    "import w2_unittest\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sF9Hqzgwt0l"
   },
   "source": [
    "<a name='1'></a>\n",
    "# Part 1: Importing the Data\n",
    "\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Loading in the data\n",
    "\n",
    "<img src = \"images/shakespeare.png\" style=\"width:250px;height:250px;\"/>\n",
    "\n",
    "Now import the dataset and do some processing. \n",
    "- The dataset has one sentence per line.\n",
    "- You will be doing character generation, so you have to process each sentence by converting each **character** (and not word) to a number. \n",
    "- You will use the `ord` function to convert a unique character to a unique integer ID. \n",
    "- Store each line in a list.\n",
    "- Create a data generator that takes in the `batch_size` and the `max_length`. \n",
    "    - The `max_length` corresponds to the maximum length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        # remove leading and trailing whitespace\n",
    "        pure_line = line.strip()\n",
    "\n",
    "        # if pure_line is not the empty string,\n",
    "        if pure_line:\n",
    "            # append it to the list\n",
    "            lines.append(pure_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A LOVER'S COMPLAINT\",\n",
       " 'FROM off a hill whose concave womb reworded',\n",
       " 'A plaintful story from a sistering vale,',\n",
       " 'My spirits to attend this double voice accorded,',\n",
       " 'And down I laid to list the sad-tuned tale;',\n",
       " 'Ere long espied a fickle maid full pale,',\n",
       " 'Tearing of papers, breaking rings a-twain,',\n",
       " \"Storming her world with sorrow's wind and rain.\",\n",
       " 'Upon her head a platted hive of straw,',\n",
       " 'Which fortified her visage from the sun,',\n",
       " 'Whereon the thought might think sometime it saw',\n",
       " 'The carcass of beauty spent and done:',\n",
       " 'Time had not scythed all that youth begun,',\n",
       " \"Nor youth all quit; but, spite of heaven's fell rage,\",\n",
       " \"Some beauty peep'd through lattice of sear'd age.\",\n",
       " 'Oft did she heave her napkin to her eyne,',\n",
       " 'Which on it had conceited characters,',\n",
       " 'Laundering the silken figures in the brine',\n",
       " \"That season'd woe had pelleted in tears,\",\n",
       " 'And often reading what contents it bears;',\n",
       " \"As often shrieking undistinguish'd woe,\",\n",
       " 'In clamours of all size, both high and low.',\n",
       " \"Sometimes her levell'd eyes their carriage ride,\",\n",
       " 'As they did battery to the spheres intend;',\n",
       " 'Sometime diverted their poor balls are tied',\n",
       " 'To the orbed earth; sometimes they do extend',\n",
       " 'Their view right on; anon their gazes lend',\n",
       " \"To every place at once, and, nowhere fix'd,\",\n",
       " \"The mind and sight distractedly commix'd.\",\n",
       " 'Her hair, nor loose nor tied in formal plat,',\n",
       " \"Proclaim'd in her a careless hand of pride\",\n",
       " \"For some, untuck'd, descended her sheaved hat,\",\n",
       " 'Hanging her pale and pined cheek beside;',\n",
       " 'Some in her threaden fillet still did bide,',\n",
       " 'And true to bondage would not break from thence,',\n",
       " 'Though slackly braided in loose negligence.',\n",
       " 'A thousand favours from a maund she drew',\n",
       " 'Of amber, crystal, and of beaded jet,',\n",
       " 'Which one by one she in a river threw,',\n",
       " 'Upon whose weeping margent she was set;',\n",
       " 'Like usury, applying wet to wet,',\n",
       " \"Or monarch's hands that let not bounty fall\",\n",
       " 'Where want cries some, but where excess begs all.',\n",
       " 'Of folded schedules had she many a one,',\n",
       " \"Which she perused, sigh'd, tore, and gave the flood;\",\n",
       " \"Crack'd many a ring of posied gold and bone\",\n",
       " 'Bidding them find their sepulchres in mud;',\n",
       " \"Found yet moe letters sadly penn'd in blood,\",\n",
       " 'With sleided silk feat and affectedly',\n",
       " \"Enswathed, and seal'd to curious secrecy.\",\n",
       " 'These often bathed she in her fluxive eyes,',\n",
       " \"And often kiss'd, and often 'gan to tear:\",\n",
       " \"Cried 'O false blood, thou register of lies,\",\n",
       " 'What unapproved witness dost thou bear!',\n",
       " \"Ink would have seem'd more black and damned here!'\",\n",
       " 'This said, in top of rage the lines she rents,',\n",
       " 'Big discontent so breaking their contents.',\n",
       " 'A reverend man that grazed his cattle nigh--',\n",
       " 'Sometime a blusterer, that the ruffle knew',\n",
       " 'Of court, of city, and had let go by',\n",
       " 'The swiftest hours, observed as they flew--',\n",
       " 'Towards this afflicted fancy fastly drew,',\n",
       " 'And, privileged by age, desires to know',\n",
       " 'In brief the grounds and motives of her woe.',\n",
       " 'So slides he down upon his grained bat,',\n",
       " 'And comely-distant sits he by her side;',\n",
       " 'When he again desires her, being sat,',\n",
       " 'Her grievance with his hearing to divide:',\n",
       " 'If that from him there may be aught applied',\n",
       " 'Which may her suffering ecstasy assuage,',\n",
       " \"'Tis promised in the charity of age.\",\n",
       " \"'Father,' she says, 'though in me you behold\",\n",
       " 'The injury of many a blasting hour,',\n",
       " 'Let it not tell your judgment I am old;',\n",
       " 'Not age, but sorrow, over me hath power:',\n",
       " 'I might as yet have been a spreading flower,',\n",
       " 'Fresh to myself, If I had self-applied',\n",
       " 'Love to myself and to no love beside.',\n",
       " \"'But, woe is me! too early I attended\",\n",
       " 'A youthful suit--it was to gain my grace--',\n",
       " \"Of one by nature's outwards so commended,\",\n",
       " \"That maidens' eyes stuck over all his face:\",\n",
       " \"Love lack'd a dwelling, and made him her place;\",\n",
       " 'And when in his fair parts she did abide,',\n",
       " 'She was new lodged and newly deified.',\n",
       " \"'His browny locks did hang in crooked curls;\",\n",
       " 'And every light occasion of the wind',\n",
       " 'Upon his lips their silken parcels hurls.',\n",
       " \"What's sweet to do, to do will aptly find:\",\n",
       " 'Each eye that saw him did enchant the mind,',\n",
       " 'For on his visage was in little drawn',\n",
       " 'What largeness thinks in Paradise was sawn.',\n",
       " \"'Small show of man was yet upon his chin;\",\n",
       " 'His phoenix down began but to appear',\n",
       " 'Like unshorn velvet on that termless skin',\n",
       " \"Whose bare out-bragg'd the web it seem'd to wear:\",\n",
       " \"Yet show'd his visage by that cost more dear;\",\n",
       " 'And nice affections wavering stood in doubt',\n",
       " 'If best were as it was, or best without.',\n",
       " \"'His qualities were beauteous as his form,\",\n",
       " 'For maiden-tongued he was, and thereof free;',\n",
       " 'Yet, if men moved him, was he such a storm',\n",
       " \"As oft 'twixt May and April is to see,\",\n",
       " 'When winds breathe sweet, untidy though they be.',\n",
       " 'His rudeness so with his authorized youth',\n",
       " 'Did livery falseness in a pride of truth.',\n",
       " \"'Well could he ride, and often men would say\",\n",
       " \"'That horse his mettle from his rider takes:\",\n",
       " 'Proud of subjection, noble by the sway,',\n",
       " 'What rounds, what bounds, what course, what stop',\n",
       " \"he makes!'\",\n",
       " 'And controversy hence a question takes,',\n",
       " 'Whether the horse by him became his deed,',\n",
       " 'Or he his manage by the well-doing steed.',\n",
       " \"'But quickly on this side the verdict went:\",\n",
       " 'His real habitude gave life and grace',\n",
       " 'To appertainings and to ornament,',\n",
       " \"Accomplish'd in himself, not in his case:\",\n",
       " 'All aids, themselves made fairer by their place,',\n",
       " 'Came for additions; yet their purposed trim',\n",
       " 'Pieced not his grace, but were all graced by him.',\n",
       " \"'So on the tip of his subduing tongue\",\n",
       " 'All kinds of arguments and question deep,',\n",
       " 'All replication prompt, and reason strong,',\n",
       " 'For his advantage still did wake and sleep:',\n",
       " 'To make the weeper laugh, the laugher weep,',\n",
       " 'He had the dialect and different skill,',\n",
       " 'Catching all passions in his craft of will:',\n",
       " \"'That he did in the general bosom reign\",\n",
       " 'Of young, of old; and sexes both enchanted,',\n",
       " 'To dwell with him in thoughts, or to remain',\n",
       " 'In personal duty, following where he haunted:',\n",
       " \"Consents bewitch'd, ere he desire, have granted;\",\n",
       " 'And dialogued for him what he would say,',\n",
       " \"Ask'd their own wills, and made their wills obey.\",\n",
       " \"'Many there were that did his picture get,\",\n",
       " 'To serve their eyes, and in it put their mind;',\n",
       " \"Like fools that in th' imagination set\",\n",
       " 'The goodly objects which abroad they find',\n",
       " \"Of lands and mansions, theirs in thought assign'd;\",\n",
       " 'And labouring in moe pleasures to bestow them',\n",
       " 'Than the true gouty landlord which doth owe them:',\n",
       " \"'So many have, that never touch'd his hand,\",\n",
       " 'Sweetly supposed them mistress of his heart.',\n",
       " 'My woeful self, that did in freedom stand,',\n",
       " 'And was my own fee-simple, not in part,',\n",
       " 'What with his art in youth, and youth in art,',\n",
       " 'Threw my affections in his charmed power,',\n",
       " 'Reserved the stalk and gave him all my flower.',\n",
       " \"'Yet did I not, as some my equals did,\",\n",
       " 'Demand of him, nor being desired yielded;',\n",
       " 'Finding myself in honour so forbid,',\n",
       " 'With safest distance I mine honour shielded:',\n",
       " 'Experience for me many bulwarks builded',\n",
       " \"Of proofs new-bleeding, which remain'd the foil\",\n",
       " 'Of this false jewel, and his amorous spoil.',\n",
       " \"'But, ah, who ever shunn'd by precedent\",\n",
       " 'The destined ill she must herself assay?',\n",
       " \"Or forced examples, 'gainst her own content,\",\n",
       " 'To put the by-past perils in her way?',\n",
       " 'Counsel may stop awhile what will not stay;',\n",
       " 'For when we rage, advice is often seen',\n",
       " 'By blunting us to make our wits more keen.',\n",
       " \"'Nor gives it satisfaction to our blood,\",\n",
       " \"That we must curb it upon others' proof;\",\n",
       " 'To be forbod the sweets that seem so good,',\n",
       " 'For fear of harms that preach in our behoof.',\n",
       " 'O appetite, from judgment stand aloof!',\n",
       " 'The one a palate hath that needs will taste,',\n",
       " \"Though Reason weep, and cry, 'It is thy last.'\",\n",
       " \"'For further I could say 'This man's untrue,'\",\n",
       " 'And knew the patterns of his foul beguiling;',\n",
       " \"Heard where his plants in others' orchards grew,\",\n",
       " 'Saw how deceits were gilded in his smiling;',\n",
       " 'Knew vows were ever brokers to defiling;',\n",
       " 'Thought characters and words merely but art,',\n",
       " 'And bastards of his foul adulterate heart.',\n",
       " \"'And long upon these terms I held my city,\",\n",
       " \"Till thus he gan besiege me: 'Gentle maid,\",\n",
       " 'Have of my suffering youth some feeling pity,',\n",
       " 'And be not of my holy vows afraid:',\n",
       " \"That's to ye sworn to none was ever said;\",\n",
       " \"For feasts of love I have been call'd unto,\",\n",
       " \"Till now did ne'er invite, nor never woo.\",\n",
       " \"''All my offences that abroad you see\",\n",
       " 'Are errors of the blood, none of the mind;',\n",
       " 'Love made them not: with acture they may be,',\n",
       " 'Where neither party is nor true nor kind:',\n",
       " 'They sought their shame that so their shame did find;',\n",
       " 'And so much less of shame in me remains,',\n",
       " 'By how much of me their reproach contains.',\n",
       " \"''Among the many that mine eyes have seen,\",\n",
       " \"Not one whose flame my heart so much as warm'd,\",\n",
       " 'Or my affection put to the smallest teen,',\n",
       " \"Or any of my leisures ever charm'd:\",\n",
       " \"Harm have I done to them, but ne'er was harm'd;\",\n",
       " 'Kept hearts in liveries, but mine own was free,',\n",
       " \"And reign'd, commanding in his monarchy.\",\n",
       " \"''Look here, what tributes wounded fancies sent me,\",\n",
       " 'Of paled pearls and rubies red as blood;',\n",
       " 'Figuring that they their passions likewise lent me',\n",
       " 'Of grief and blushes, aptly understood',\n",
       " \"In bloodless white and the encrimson'd mood;\",\n",
       " 'Effects of terror and dear modesty,',\n",
       " \"Encamp'd in hearts, but fighting outwardly.\",\n",
       " \"''And, lo, behold these talents of their hair,\",\n",
       " \"With twisted metal amorously impleach'd,\",\n",
       " 'I have received from many a several fair,',\n",
       " \"Their kind acceptance weepingly beseech'd,\",\n",
       " \"With the annexions of fair gems enrich'd,\",\n",
       " \"And deep-brain'd sonnets that did amplify\",\n",
       " \"Each stone's dear nature, worth, and quality.\",\n",
       " \"''The diamond,--why, 'twas beautiful and hard,\",\n",
       " 'Whereto his invised properties did tend;',\n",
       " 'The deep-green emerald, in whose fresh regard',\n",
       " 'Weak sights their sickly radiance do amend;',\n",
       " 'The heaven-hued sapphire and the opal blend',\n",
       " 'With objects manifold: each several stone,',\n",
       " \"With wit well blazon'd, smiled or made some moan.\",\n",
       " \"''Lo, all these trophies of affections hot,\",\n",
       " 'Of pensived and subdued desires the tender,',\n",
       " 'Nature hath charged me that I hoard them not,',\n",
       " 'But yield them up where I myself must render,',\n",
       " 'That is, to you, my origin and ender;',\n",
       " 'For these, of force, must your oblations be,',\n",
       " 'Since I their altar, you enpatron me.',\n",
       " \"''O, then, advance of yours that phraseless hand,\",\n",
       " 'Whose white weighs down the airy scale of praise;',\n",
       " 'Take all these similes to your own command,',\n",
       " \"Hallow'd with sighs that burning lungs did raise;\",\n",
       " 'What me your minister, for you obeys,',\n",
       " 'Works under you; and to your audit comes',\n",
       " 'Their distract parcels in combined sums.',\n",
       " \"''Lo, this device was sent me from a nun,\",\n",
       " 'Or sister sanctified, of holiest note;',\n",
       " 'Which late her noble suit in court did shun,',\n",
       " 'Whose rarest havings made the blossoms dote;',\n",
       " 'For she was sought by spirits of richest coat,',\n",
       " 'But kept cold distance, and did thence remove,',\n",
       " 'To spend her living in eternal love.',\n",
       " \"''But, O my sweet, what labour is't to leave\",\n",
       " 'The thing we have not, mastering what not strives,',\n",
       " 'Playing the place which did no form receive,',\n",
       " 'Playing patient sports in unconstrained gyves?',\n",
       " 'She that her fame so to herself contrives,',\n",
       " \"The scars of battle 'scapeth by the flight,\",\n",
       " 'And makes her absence valiant, not her might.',\n",
       " \"''O, pardon me, in that my boast is true:\",\n",
       " 'The accident which brought me to her eye',\n",
       " 'Upon the moment did her force subdue,',\n",
       " 'And now she would the caged cloister fly:',\n",
       " \"Religious love put out Religion's eye:\",\n",
       " 'Not to be tempted, would she be immured,',\n",
       " 'And now, to tempt, all liberty procured.',\n",
       " \"''How mighty then you are, O, hear me tell!\",\n",
       " 'The broken bosoms that to me belong',\n",
       " 'Have emptied all their fountains in my well,',\n",
       " 'And mine I pour your ocean all among:',\n",
       " \"I strong o'er them, and you o'er me being strong,\",\n",
       " 'Must for your victory us all congest,',\n",
       " 'As compound love to physic your cold breast.',\n",
       " \"''My parts had power to charm a sacred nun,\",\n",
       " 'Who, disciplined, ay, dieted in grace,',\n",
       " 'Believed her eyes when they to assail begun,',\n",
       " 'All vows and consecrations giving place:',\n",
       " 'O most potential love! vow, bond, nor space,',\n",
       " 'In thee hath neither sting, knot, nor confine,',\n",
       " 'For thou art all, and all things else are thine.',\n",
       " \"''When thou impressest, what are precepts worth\",\n",
       " 'Of stale example? When thou wilt inflame,',\n",
       " 'How coldly those impediments stand forth',\n",
       " 'Of wealth, of filial fear, law, kindred, fame!',\n",
       " \"Love's arms are peace, 'gainst rule, 'gainst sense,\",\n",
       " \"'gainst shame,\",\n",
       " 'And sweetens, in the suffering pangs it bears,',\n",
       " 'The aloes of all forces, shocks, and fears.',\n",
       " \"''Now all these hearts that do on mine depend,\",\n",
       " 'Feeling it break, with bleeding groans they pine;',\n",
       " 'And supplicant their sighs to you extend,',\n",
       " \"To leave the battery that you make 'gainst mine,\",\n",
       " 'Lending soft audience to my sweet design,',\n",
       " 'And credent soul to that strong-bonded oath',\n",
       " \"That shall prefer and undertake my troth.'\",\n",
       " \"'This said, his watery eyes he did dismount,\",\n",
       " \"Whose sights till then were levell'd on my face;\",\n",
       " 'Each cheek a river running from a fount',\n",
       " \"With brinish current downward flow'd apace:\",\n",
       " 'O, how the channel to the stream gave grace!',\n",
       " 'Who glazed with crystal gate the glowing roses',\n",
       " 'That flame through water which their hue encloses.',\n",
       " \"'O father, what a hell of witchcraft lies\",\n",
       " 'In the small orb of one particular tear!',\n",
       " 'But with the inundation of the eyes',\n",
       " 'What rocky heart to water will not wear?',\n",
       " 'What breast so cold that is not warmed here?',\n",
       " 'O cleft effect! cold modesty, hot wrath,',\n",
       " 'Both fire from hence and chill extincture hath.',\n",
       " \"'For, lo, his passion, but an art of craft,\",\n",
       " 'Even there resolved my reason into tears;',\n",
       " \"There my white stole of chastity I daff'd,\",\n",
       " 'Shook off my sober guards and civil fears;',\n",
       " 'Appear to him, as he to me appears,',\n",
       " 'All melting; though our drops this difference bore,',\n",
       " \"His poison'd me, and mine did him restore.\",\n",
       " \"'In him a plenitude of subtle matter,\",\n",
       " 'Applied to cautels, all strange forms receives,',\n",
       " 'Of burning blushes, or of weeping water,',\n",
       " 'Or swooning paleness; and he takes and leaves,',\n",
       " \"In either's aptness, as it best deceives,\",\n",
       " 'To blush at speeches rank to weep at woes,',\n",
       " 'Or to turn white and swoon at tragic shows.',\n",
       " \"'That not a heart which in his level came\",\n",
       " \"Could 'scape the hail of his all-hurting aim,\",\n",
       " 'Showing fair nature is both kind and tame;',\n",
       " \"And, veil'd in them, did win whom he would maim:\",\n",
       " 'Against the thing he sought he would exclaim;',\n",
       " \"When he most burn'd in heart-wish'd luxury,\",\n",
       " \"He preach'd pure maid, and praised cold chastity.\",\n",
       " \"'Thus merely with the garment of a Grace\",\n",
       " \"The naked and concealed fiend he cover'd;\",\n",
       " \"That th' unexperient gave the tempter place,\",\n",
       " \"Which like a cherubin above them hover'd.\",\n",
       " \"Who, young and simple, would not be so lover'd?\",\n",
       " 'Ay me! I fell; and yet do question make',\n",
       " 'What I should do again for such a sake.',\n",
       " \"'O, that infected moisture of his eye,\",\n",
       " \"O, that false fire which in his cheek so glow'd,\",\n",
       " 'O, that forced thunder from his heart did fly,',\n",
       " \"O, that sad breath his spongy lungs bestow'd,\",\n",
       " \"O, all that borrow'd motion seeming owed,\",\n",
       " \"Would yet again betray the fore-betray'd,\",\n",
       " \"And new pervert a reconciled maid!'\",\n",
       " 'ROMEO AND JULIET',\n",
       " 'DRAMATIS PERSONAE',\n",
       " 'ESCALUS\\tprince of Verona. (PRINCE:)',\n",
       " 'PARIS\\ta young nobleman, kinsman to the prince.',\n",
       " 'MONTAGUE\\t|',\n",
       " '|  heads of two houses at variance with each other.',\n",
       " 'CAPULET\\t|',\n",
       " 'An old man, cousin to Capulet. (Second Capulet:)',\n",
       " 'ROMEO\\tson to Montague.',\n",
       " 'MERCUTIO\\tkinsman to the prince, and friend to Romeo.',\n",
       " 'BENVOLIO\\tnephew to Montague, and friend to Romeo.',\n",
       " 'TYBALT\\tnephew to Lady Capulet.',\n",
       " 'FRIAR LAURENCE\\t|',\n",
       " '|  Franciscans.',\n",
       " 'FRIAR JOHN\\t|',\n",
       " 'BALTHASAR\\tservant to Romeo.',\n",
       " 'SAMPSON\\t|',\n",
       " '|  servants to Capulet.',\n",
       " 'GREGORY\\t|',\n",
       " \"PETER\\tservant to Juliet's nurse.\",\n",
       " 'ABRAHAM\\tservant to Montague.',\n",
       " 'An Apothecary. (Apothecary:)',\n",
       " 'Three Musicians.',\n",
       " '(First Musician:)',\n",
       " '(Second Musician:)',\n",
       " '(Third Musician:)',\n",
       " 'Page to Paris; (PAGE:)  another Page; an officer.',\n",
       " 'LADY MONTAGUE\\twife to Montague.',\n",
       " 'LADY CAPULET\\twife to Capulet.',\n",
       " 'JULIET\\tdaughter to Capulet.',\n",
       " 'Nurse to Juliet. (Nurse:)',\n",
       " 'Citizens of Verona; several Men and Women,',\n",
       " 'relations to both houses; Maskers,',\n",
       " 'Guards, Watchmen, and Attendants.',\n",
       " '(First Citizen:)',\n",
       " '(Servant:)',\n",
       " '(First Servant:)',\n",
       " '(Second Servant:)',\n",
       " '(First Watchman:)',\n",
       " '(Second Watchman:)',\n",
       " '(Third Watchman:)',\n",
       " 'Chorus.',\n",
       " 'SCENE\\tVerona: Mantua.',\n",
       " 'ROMEO AND JULIET',\n",
       " 'PROLOGUE',\n",
       " 'Two households, both alike in dignity,',\n",
       " 'In fair Verona, where we lay our scene,',\n",
       " 'From ancient grudge break to new mutiny,',\n",
       " 'Where civil blood makes civil hands unclean.',\n",
       " 'From forth the fatal loins of these two foes',\n",
       " \"A pair of star-cross'd lovers take their life;\",\n",
       " 'Whole misadventured piteous overthrows',\n",
       " \"Do with their death bury their parents' strife.\",\n",
       " \"The fearful passage of their death-mark'd love,\",\n",
       " \"And the continuance of their parents' rage,\",\n",
       " \"Which, but their children's end, nought could remove,\",\n",
       " \"Is now the two hours' traffic of our stage;\",\n",
       " 'The which if you with patient ears attend,',\n",
       " 'What here shall miss, our toil shall strive to mend.',\n",
       " 'ROMEO AND JULIET',\n",
       " 'ACT I',\n",
       " 'SCENE I\\tVerona. A public place.',\n",
       " '[Enter SAMPSON and GREGORY, of the house of Capulet,',\n",
       " 'armed with swords and bucklers]',\n",
       " \"SAMPSON\\tGregory, o' my word, we'll not carry coals.\",\n",
       " 'GREGORY\\tNo, for then we should be colliers.',\n",
       " \"SAMPSON\\tI mean, an we be in choler, we'll draw.\",\n",
       " \"GREGORY\\tAy, while you live, draw your neck out o' the collar.\",\n",
       " 'SAMPSON\\tI strike quickly, being moved.',\n",
       " 'GREGORY\\tBut thou art not quickly moved to strike.',\n",
       " 'SAMPSON\\tA dog of the house of Montague moves me.',\n",
       " 'GREGORY\\tTo move is to stir; and to be valiant is to stand:',\n",
       " \"therefore, if thou art moved, thou runn'st away.\",\n",
       " 'SAMPSON\\tA dog of that house shall move me to stand: I will',\n",
       " \"take the wall of any man or maid of Montague's.\",\n",
       " 'GREGORY\\tThat shows thee a weak slave; for the weakest goes',\n",
       " 'to the wall.',\n",
       " 'SAMPSON\\tTrue; and therefore women, being the weaker vessels,',\n",
       " 'are ever thrust to the wall: therefore I will push',\n",
       " \"Montague's men from the wall, and thrust his maids\",\n",
       " 'to the wall.',\n",
       " 'GREGORY\\tThe quarrel is between our masters and us their men.',\n",
       " \"SAMPSON\\t'Tis all one, I will show myself a tyrant: when I\",\n",
       " 'have fought with the men, I will be cruel with the',\n",
       " 'maids, and cut off their heads.',\n",
       " 'GREGORY\\tThe heads of the maids?',\n",
       " 'SAMPSON\\tAy, the heads of the maids, or their maidenheads;',\n",
       " 'take it in what sense thou wilt.',\n",
       " 'GREGORY\\tThey must take it in sense that feel it.',\n",
       " 'SAMPSON\\tMe they shall feel while I am able to stand: and',\n",
       " \"'tis known I am a pretty piece of flesh.\",\n",
       " \"GREGORY\\t'Tis well thou art not fish; if thou hadst, thou\",\n",
       " 'hadst been poor John. Draw thy tool! here comes',\n",
       " 'two of the house of the Montagues.',\n",
       " 'SAMPSON\\tMy naked weapon is out: quarrel, I will back thee.',\n",
       " 'GREGORY\\tHow! turn thy back and run?',\n",
       " 'SAMPSON\\tFear me not.',\n",
       " 'GREGORY\\tNo, marry; I fear thee!',\n",
       " 'SAMPSON\\tLet us take the law of our sides; let them begin.',\n",
       " 'GREGORY\\tI will frown as I pass by, and let them take it as',\n",
       " 'they list.',\n",
       " 'SAMPSON\\tNay, as they dare. I will bite my thumb at them;',\n",
       " 'which is a disgrace to them, if they bear it.',\n",
       " '[Enter ABRAHAM and BALTHASAR]',\n",
       " 'ABRAHAM\\tDo you bite your thumb at us, sir?',\n",
       " 'SAMPSON\\tI do bite my thumb, sir.',\n",
       " 'ABRAHAM\\tDo you bite your thumb at us, sir?',\n",
       " 'SAMPSON\\t[Aside to GREGORY]  Is the law of our side, if I say',\n",
       " 'ay?',\n",
       " 'GREGORY\\tNo.',\n",
       " 'SAMPSON\\tNo, sir, I do not bite my thumb at you, sir, but I',\n",
       " 'bite my thumb, sir.',\n",
       " 'GREGORY\\tDo you quarrel, sir?',\n",
       " 'ABRAHAM\\tQuarrel sir! no, sir.',\n",
       " 'SAMPSON\\tIf you do, sir, I am for you: I serve as good a man as you.',\n",
       " 'ABRAHAM\\tNo better.',\n",
       " 'SAMPSON\\tWell, sir.',\n",
       " \"GREGORY\\tSay 'better:' here comes one of my master's kinsmen.\",\n",
       " 'SAMPSON\\tYes, better, sir.',\n",
       " 'ABRAHAM\\tYou lie.',\n",
       " 'SAMPSON\\tDraw, if you be men. Gregory, remember thy swashing blow.',\n",
       " '[They fight]',\n",
       " '[Enter BENVOLIO]',\n",
       " 'BENVOLIO\\tPart, fools!',\n",
       " 'Put up your swords; you know not what you do.',\n",
       " '[Beats down their swords]',\n",
       " '[Enter TYBALT]',\n",
       " 'TYBALT\\tWhat, art thou drawn among these heartless hinds?',\n",
       " 'Turn thee, Benvolio, look upon thy death.',\n",
       " 'BENVOLIO\\tI do but keep the peace: put up thy sword,',\n",
       " 'Or manage it to part these men with me.',\n",
       " 'TYBALT\\tWhat, drawn, and talk of peace! I hate the word,',\n",
       " 'As I hate hell, all Montagues, and thee:',\n",
       " 'Have at thee, coward!',\n",
       " '[They fight]',\n",
       " '[Enter, several of both houses, who join the fray;',\n",
       " 'then enter Citizens, with clubs]',\n",
       " 'First Citizen\\tClubs, bills, and partisans! strike! beat them down!',\n",
       " 'Down with the Capulets! down with the Montagues!',\n",
       " '[Enter CAPULET in his gown, and LADY CAPULET]',\n",
       " 'CAPULET\\tWhat noise is this? Give me my long sword, ho!',\n",
       " 'LADY CAPULET\\tA crutch, a crutch! why call you for a sword?',\n",
       " 'CAPULET\\tMy sword, I say! Old Montague is come,',\n",
       " 'And flourishes his blade in spite of me.',\n",
       " '[Enter MONTAGUE and LADY MONTAGUE]',\n",
       " 'MONTAGUE\\tThou villain Capulet,--Hold me not, let me go.',\n",
       " 'LADY MONTAGUE\\tThou shalt not stir a foot to seek a foe.',\n",
       " '[Enter PRINCE, with Attendants]',\n",
       " 'PRINCE\\tRebellious subjects, enemies to peace,',\n",
       " 'Profaners of this neighbour-stained steel,--',\n",
       " 'Will they not hear? What, ho! you men, you beasts,',\n",
       " 'That quench the fire of your pernicious rage',\n",
       " 'With purple fountains issuing from your veins,',\n",
       " 'On pain of torture, from those bloody hands',\n",
       " \"Throw your mistemper'd weapons to the ground,\",\n",
       " 'And hear the sentence of your moved prince.',\n",
       " 'Three civil brawls, bred of an airy word,',\n",
       " 'By thee, old Capulet, and Montague,',\n",
       " \"Have thrice disturb'd the quiet of our streets,\",\n",
       " \"And made Verona's ancient citizens\",\n",
       " 'Cast by their grave beseeming ornaments,',\n",
       " 'To wield old partisans, in hands as old,',\n",
       " \"Canker'd with peace, to part your canker'd hate:\",\n",
       " 'If ever you disturb our streets again,',\n",
       " 'Your lives shall pay the forfeit of the peace.',\n",
       " 'For this time, all the rest depart away:',\n",
       " 'You Capulet; shall go along with me:',\n",
       " 'And, Montague, come you this afternoon,',\n",
       " 'To know our further pleasure in this case,',\n",
       " 'To old Free-town, our common judgment-place.',\n",
       " 'Once more, on pain of death, all men depart.',\n",
       " '[Exeunt all but MONTAGUE, LADY MONTAGUE, and BENVOLIO]',\n",
       " 'MONTAGUE\\tWho set this ancient quarrel new abroach?',\n",
       " 'Speak, nephew, were you by when it began?',\n",
       " 'BENVOLIO\\tHere were the servants of your adversary,',\n",
       " 'And yours, close fighting ere I did approach:',\n",
       " 'I drew to part them: in the instant came',\n",
       " 'The fiery Tybalt, with his sword prepared,',\n",
       " 'Which, as he breathed defiance to my ears,',\n",
       " 'He swung about his head and cut the winds,',\n",
       " \"Who nothing hurt withal hiss'd him in scorn:\",\n",
       " 'While we were interchanging thrusts and blows,',\n",
       " 'Came more and more and fought on part and part,',\n",
       " 'Till the prince came, who parted either part.',\n",
       " 'LADY MONTAGUE\\tO, where is Romeo? saw you him to-day?',\n",
       " 'Right glad I am he was not at this fray.',\n",
       " \"BENVOLIO\\tMadam, an hour before the worshipp'd sun\",\n",
       " \"Peer'd forth the golden window of the east,\",\n",
       " 'A troubled mind drave me to walk abroad;',\n",
       " 'Where, underneath the grove of sycamore',\n",
       " \"That westward rooteth from the city's side,\",\n",
       " 'So early walking did I see your son:',\n",
       " 'Towards him I made, but he was ware of me',\n",
       " 'And stole into the covert of the wood:',\n",
       " 'I, measuring his affections by my own,',\n",
       " \"That most are busied when they're most alone,\",\n",
       " 'Pursued my humour not pursuing his,',\n",
       " \"And gladly shunn'd who gladly fled from me.\",\n",
       " 'MONTAGUE\\tMany a morning hath he there been seen,',\n",
       " 'With tears augmenting the fresh morning dew.',\n",
       " 'Adding to clouds more clouds with his deep sighs;',\n",
       " 'But all so soon as the all-cheering sun',\n",
       " 'Should in the furthest east begin to draw',\n",
       " \"The shady curtains from Aurora's bed,\",\n",
       " 'Away from the light steals home my heavy son,',\n",
       " 'And private in his chamber pens himself,',\n",
       " 'Shuts up his windows, locks far daylight out',\n",
       " 'And makes himself an artificial night:',\n",
       " 'Black and portentous must this humour prove,',\n",
       " 'Unless good counsel may the cause remove.',\n",
       " 'BENVOLIO\\tMy noble uncle, do you know the cause?',\n",
       " 'MONTAGUE\\tI neither know it nor can learn of him.',\n",
       " 'BENVOLIO\\tHave you importuned him by any means?',\n",
       " 'MONTAGUE\\tBoth by myself and many other friends:',\n",
       " \"But he, his own affections' counsellor,\",\n",
       " 'Is to himself--I will not say how true--',\n",
       " 'But to himself so secret and so close,',\n",
       " 'So far from sounding and discovery,',\n",
       " 'As is the bud bit with an envious worm,',\n",
       " 'Ere he can spread his sweet leaves to the air,',\n",
       " 'Or dedicate his beauty to the sun.',\n",
       " 'Could we but learn from whence his sorrows grow.',\n",
       " 'We would as willingly give cure as know.',\n",
       " '[Enter ROMEO]',\n",
       " 'BENVOLIO\\tSee, where he comes: so please you, step aside;',\n",
       " \"I'll know his grievance, or be much denied.\",\n",
       " 'MONTAGUE\\tI would thou wert so happy by thy stay,',\n",
       " \"To hear true shrift. Come, madam, let's away.\",\n",
       " '[Exeunt MONTAGUE and LADY MONTAGUE]',\n",
       " 'BENVOLIO\\tGood-morrow, cousin.',\n",
       " 'ROMEO\\tIs the day so young?',\n",
       " 'BENVOLIO\\tBut new struck nine.',\n",
       " 'ROMEO\\tAy me! sad hours seem long.',\n",
       " 'Was that my father that went hence so fast?',\n",
       " \"BENVOLIO\\tIt was. What sadness lengthens Romeo's hours?\",\n",
       " 'ROMEO\\tNot having that, which, having, makes them short.',\n",
       " 'BENVOLIO\\tIn love?',\n",
       " 'ROMEO\\tOut--',\n",
       " 'BENVOLIO\\tOf love?',\n",
       " 'ROMEO\\tOut of her favour, where I am in love.',\n",
       " 'BENVOLIO\\tAlas, that love, so gentle in his view,',\n",
       " 'Should be so tyrannous and rough in proof!',\n",
       " 'ROMEO\\tAlas, that love, whose view is muffled still,',\n",
       " 'Should, without eyes, see pathways to his will!',\n",
       " 'Where shall we dine? O me! What fray was here?',\n",
       " 'Yet tell me not, for I have heard it all.',\n",
       " \"Here's much to do with hate, but more with love.\",\n",
       " 'Why, then, O brawling love! O loving hate!',\n",
       " 'O any thing, of nothing first create!',\n",
       " 'O heavy lightness! serious vanity!',\n",
       " 'Mis-shapen chaos of well-seeming forms!',\n",
       " 'Feather of lead, bright smoke, cold fire,',\n",
       " 'sick health!',\n",
       " 'Still-waking sleep, that is not what it is!',\n",
       " 'This love feel I, that feel no love in this.',\n",
       " 'Dost thou not laugh?',\n",
       " 'BENVOLIO\\tNo, coz, I rather weep.',\n",
       " 'ROMEO\\tGood heart, at what?',\n",
       " \"BENVOLIO\\tAt thy good heart's oppression.\",\n",
       " \"ROMEO\\tWhy, such is love's transgression.\",\n",
       " 'Griefs of mine own lie heavy in my breast,',\n",
       " 'Which thou wilt propagate, to have it prest',\n",
       " 'With more of thine: this love that thou hast shown',\n",
       " 'Doth add more grief to too much of mine own.',\n",
       " 'Love is a smoke raised with the fume of sighs;',\n",
       " \"Being purged, a fire sparkling in lovers' eyes;\",\n",
       " \"Being vex'd a sea nourish'd with lovers' tears:\",\n",
       " 'What is it else? a madness most discreet,',\n",
       " 'A choking gall and a preserving sweet.',\n",
       " 'Farewell, my coz.',\n",
       " 'BENVOLIO\\t                  Soft! I will go along;',\n",
       " 'An if you leave me so, you do me wrong.',\n",
       " 'ROMEO\\tTut, I have lost myself; I am not here;',\n",
       " \"This is not Romeo, he's some other where.\",\n",
       " 'BENVOLIO\\tTell me in sadness, who is that you love.',\n",
       " 'ROMEO\\tWhat, shall I groan and tell thee?',\n",
       " 'BENVOLIO\\tGroan! why, no.',\n",
       " 'But sadly tell me who.',\n",
       " 'ROMEO\\tBid a sick man in sadness make his will:',\n",
       " 'Ah, word ill urged to one that is so ill!',\n",
       " 'In sadness, cousin, I do love a woman.',\n",
       " \"BENVOLIO\\tI aim'd so near, when I supposed you loved.\",\n",
       " \"ROMEO\\tA right good mark-man! And she's fair I love.\",\n",
       " 'BENVOLIO\\tA right fair mark, fair coz, is soonest hit.',\n",
       " \"ROMEO\\tWell, in that hit you miss: she'll not be hit\",\n",
       " \"With Cupid's arrow; she hath Dian's wit;\",\n",
       " \"And, in strong proof of chastity well arm'd,\",\n",
       " \"From love's weak childish bow she lives unharm'd.\",\n",
       " 'She will not stay the siege of loving terms,',\n",
       " 'Nor bide the encounter of assailing eyes,',\n",
       " 'Nor ope her lap to saint-seducing gold:',\n",
       " 'O, she is rich in beauty, only poor,',\n",
       " 'That when she dies with beauty dies her store.',\n",
       " 'BENVOLIO\\tThen she hath sworn that she will still live chaste?',\n",
       " 'ROMEO\\tShe hath, and in that sparing makes huge waste,',\n",
       " 'For beauty starved with her severity',\n",
       " 'Cuts beauty off from all posterity.',\n",
       " 'She is too fair, too wise, wisely too fair,',\n",
       " 'To merit bliss by making me despair:',\n",
       " 'She hath forsworn to love, and in that vow',\n",
       " 'Do I live dead that live to tell it now.',\n",
       " 'BENVOLIO\\tBe ruled by me, forget to think of her.',\n",
       " 'ROMEO\\tO, teach me how I should forget to think.',\n",
       " 'BENVOLIO\\tBy giving liberty unto thine eyes;',\n",
       " 'Examine other beauties.',\n",
       " \"ROMEO\\t'Tis the way\",\n",
       " 'To call hers exquisite, in question more:',\n",
       " \"These happy masks that kiss fair ladies' brows\",\n",
       " 'Being black put us in mind they hide the fair;',\n",
       " 'He that is strucken blind cannot forget',\n",
       " 'The precious treasure of his eyesight lost:',\n",
       " 'Show me a mistress that is passing fair,',\n",
       " 'What doth her beauty serve, but as a note',\n",
       " \"Where I may read who pass'd that passing fair?\",\n",
       " 'Farewell: thou canst not teach me to forget.',\n",
       " \"BENVOLIO\\tI'll pay that doctrine, or else die in debt.\",\n",
       " '[Exeunt]',\n",
       " 'ROMEO AND JULIET',\n",
       " 'ACT I',\n",
       " 'SCENE II\\tA street.',\n",
       " '[Enter CAPULET, PARIS, and Servant]',\n",
       " 'CAPULET\\tBut Montague is bound as well as I,',\n",
       " \"In penalty alike; and 'tis not hard, I think,\",\n",
       " 'For men so old as we to keep the peace.',\n",
       " 'PARIS\\tOf honourable reckoning are you both;',\n",
       " \"And pity 'tis you lived at odds so long.\",\n",
       " 'But now, my lord, what say you to my suit?',\n",
       " \"CAPULET\\tBut saying o'er what I have said before:\",\n",
       " 'My child is yet a stranger in the world;',\n",
       " 'She hath not seen the change of fourteen years,',\n",
       " 'Let two more summers wither in their pride,',\n",
       " 'Ere we may think her ripe to be a bride.',\n",
       " 'PARIS\\tYounger than she are happy mothers made.',\n",
       " \"CAPULET\\tAnd too soon marr'd are those so early made.\",\n",
       " \"The earth hath swallow'd all my hopes but she,\",\n",
       " 'She is the hopeful lady of my earth:',\n",
       " 'But woo her, gentle Paris, get her heart,',\n",
       " 'My will to her consent is but a part;',\n",
       " 'An she agree, within her scope of choice',\n",
       " 'Lies my consent and fair according voice.',\n",
       " \"This night I hold an old accustom'd feast,\",\n",
       " 'Whereto I have invited many a guest,',\n",
       " 'Such as I love; and you, among the store,',\n",
       " 'One more, most welcome, makes my number more.',\n",
       " 'At my poor house look to behold this night',\n",
       " 'Earth-treading stars that make dark heaven light:',\n",
       " 'Such comfort as do lusty young men feel',\n",
       " \"When well-apparell'd April on the heel\",\n",
       " 'Of limping winter treads, even such delight',\n",
       " 'Among fresh female buds shall you this night',\n",
       " 'Inherit at my house; hear all, all see,',\n",
       " 'And like her most whose merit most shall be:',\n",
       " 'Which on more view, of many mine being one',\n",
       " 'May stand in number, though in reckoning none,',\n",
       " 'Come, go with me.',\n",
       " '[To Servant, giving a paper]',\n",
       " 'Go, sirrah, trudge about',\n",
       " 'Through fair Verona; find those persons out',\n",
       " 'Whose names are written there, and to them say,',\n",
       " 'My house and welcome on their pleasure stay.',\n",
       " '[Exeunt CAPULET and PARIS]',\n",
       " 'Servant\\tFind them out whose names are written here! It is',\n",
       " 'written, that the shoemaker should meddle with his',\n",
       " 'yard, and the tailor with his last, the fisher with',\n",
       " 'his pencil, and the painter with his nets; but I am',\n",
       " 'sent to find those persons whose names are here',\n",
       " 'writ, and can never find what names the writing',\n",
       " 'person hath here writ. I must to the learned.--In good time.',\n",
       " '[Enter BENVOLIO and ROMEO]',\n",
       " \"BENVOLIO\\tTut, man, one fire burns out another's burning,\",\n",
       " \"One pain is lessen'd by another's anguish;\",\n",
       " 'Turn giddy, and be holp by backward turning;',\n",
       " \"One desperate grief cures with another's languish:\",\n",
       " 'Take thou some new infection to thy eye,',\n",
       " 'And the rank poison of the old will die.',\n",
       " 'ROMEO\\tYour plaintain-leaf is excellent for that.',\n",
       " 'BENVOLIO\\tFor what, I pray thee?',\n",
       " 'ROMEO\\tFor your broken shin.',\n",
       " 'BENVOLIO\\tWhy, Romeo, art thou mad?',\n",
       " 'ROMEO\\tNot mad, but bound more than a mad-man is;',\n",
       " 'Shut up in prison, kept without my food,',\n",
       " \"Whipp'd and tormented and--God-den, good fellow.\",\n",
       " \"Servant\\tGod gi' god-den. I pray, sir, can you read?\",\n",
       " 'ROMEO\\tAy, mine own fortune in my misery.',\n",
       " 'Servant\\tPerhaps you have learned it without book: but, I',\n",
       " 'pray, can you read any thing you see?',\n",
       " 'ROMEO\\tAy, if I know the letters and the language.',\n",
       " 'Servant\\tYe say honestly: rest you merry!',\n",
       " 'ROMEO\\tStay, fellow; I can read.',\n",
       " '[Reads]',\n",
       " \"'Signior Martino and his wife and daughters;\",\n",
       " 'County Anselme and his beauteous sisters; the lady',\n",
       " 'widow of Vitravio; Signior Placentio and his lovely',\n",
       " 'nieces; Mercutio and his brother Valentine; mine',\n",
       " 'uncle Capulet, his wife and daughters; my fair niece',\n",
       " 'Rosaline; Livia; Signior Valentio and his cousin',\n",
       " \"Tybalt, Lucio and the lively Helena.' A fair\",\n",
       " 'assembly: whither should they come?',\n",
       " 'Servant\\tUp.',\n",
       " 'ROMEO\\tWhither?',\n",
       " 'Servant\\tTo supper; to our house.',\n",
       " 'ROMEO\\tWhose house?',\n",
       " \"Servant\\tMy master's.\",\n",
       " \"ROMEO\\tIndeed, I should have ask'd you that before.\",\n",
       " \"Servant\\tNow I'll tell you without asking: my master is the\",\n",
       " 'great rich Capulet; and if you be not of the house',\n",
       " 'of Montagues, I pray, come and crush a cup of wine.',\n",
       " 'Rest you merry!',\n",
       " '[Exit]',\n",
       " \"BENVOLIO\\tAt this same ancient feast of Capulet's\",\n",
       " 'Sups the fair Rosaline whom thou so lovest,',\n",
       " 'With all the admired beauties of Verona:',\n",
       " 'Go thither; and, with unattainted eye,',\n",
       " 'Compare her face with some that I shall show,',\n",
       " 'And I will make thee think thy swan a crow.',\n",
       " 'ROMEO\\tWhen the devout religion of mine eye',\n",
       " 'Maintains such falsehood, then turn tears to fires;',\n",
       " \"And these, who often drown'd could never die,\",\n",
       " 'Transparent heretics, be burnt for liars!',\n",
       " 'One fairer than my love! the all-seeing sun',\n",
       " \"Ne'er saw her match since first the world begun.\",\n",
       " 'BENVOLIO\\tTut, you saw her fair, none else being by,',\n",
       " 'Herself poised with herself in either eye:',\n",
       " \"But in that crystal scales let there be weigh'd\",\n",
       " \"Your lady's love against some other maid\",\n",
       " 'That I will show you shining at this feast,',\n",
       " 'And she shall scant show well that now shows best.',\n",
       " \"ROMEO\\tI'll go along, no such sight to be shown,\",\n",
       " 'But to rejoice in splendor of mine own.',\n",
       " '[Exeunt]',\n",
       " 'ROMEO AND JULIET',\n",
       " 'ACT I',\n",
       " \"SCENE III\\tA room in Capulet's house.\",\n",
       " '[Enter LADY CAPULET and Nurse]',\n",
       " \"LADY CAPULET\\tNurse, where's my daughter? call her forth to me.\",\n",
       " 'Nurse\\tNow, by my maidenhead, at twelve year old,',\n",
       " 'I bade her come. What, lamb! what, ladybird!',\n",
       " \"God forbid! Where's this girl? What, Juliet!\",\n",
       " '[Enter JULIET]',\n",
       " 'JULIET\\tHow now! who calls?',\n",
       " 'Nurse\\tYour mother.',\n",
       " 'JULIET\\tMadam, I am here.',\n",
       " 'What is your will?',\n",
       " 'LADY CAPULET\\tThis is the matter:--Nurse, give leave awhile,',\n",
       " 'We must talk in secret:--nurse, come back again;',\n",
       " \"I have remember'd me, thou's hear our counsel.\",\n",
       " \"Thou know'st my daughter's of a pretty age.\",\n",
       " 'Nurse\\tFaith, I can tell her age unto an hour.',\n",
       " \"LADY CAPULET\\tShe's not fourteen.\",\n",
       " \"Nurse\\tI'll lay fourteen of my teeth,--\",\n",
       " 'And yet, to my teeth be it spoken, I have but four--',\n",
       " 'She is not fourteen. How long is it now',\n",
       " 'To Lammas-tide?',\n",
       " 'LADY CAPULET\\t                  A fortnight and odd days.',\n",
       " 'Nurse\\tEven or odd, of all days in the year,',\n",
       " 'Come Lammas-eve at night shall she be fourteen.',\n",
       " 'Susan and she--God rest all Christian souls!--',\n",
       " 'Were of an age: well, Susan is with God;',\n",
       " 'She was too good for me: but, as I said,',\n",
       " 'On Lammas-eve at night shall she be fourteen;',\n",
       " 'That shall she, marry; I remember it well.',\n",
       " \"'Tis since the earthquake now eleven years;\",\n",
       " \"And she was wean'd,--I never shall forget it,--\",\n",
       " 'Of all the days of the year, upon that day:',\n",
       " 'For I had then laid wormwood to my dug,',\n",
       " 'Sitting in the sun under the dove-house wall;',\n",
       " 'My lord and you were then at Mantua:--',\n",
       " 'Nay, I do bear a brain:--but, as I said,',\n",
       " 'When it did taste the wormwood on the nipple',\n",
       " 'Of my dug and felt it bitter, pretty fool,',\n",
       " 'To see it tetchy and fall out with the dug!',\n",
       " \"Shake quoth the dove-house: 'twas no need, I trow,\",\n",
       " 'To bid me trudge:',\n",
       " 'And since that time it is eleven years;',\n",
       " 'For then she could stand alone; nay, by the rood,',\n",
       " 'She could have run and waddled all about;',\n",
       " 'For even the day before, she broke her brow:',\n",
       " 'And then my husband--God be with his soul!',\n",
       " \"A' was a merry man--took up the child:\",\n",
       " \"'Yea,' quoth he, 'dost thou fall upon thy face?\",\n",
       " 'Thou wilt fall backward when thou hast more wit;',\n",
       " \"Wilt thou not, Jule?' and, by my holidame,\",\n",
       " \"The pretty wretch left crying and said 'Ay.'\",\n",
       " 'To see, now, how a jest shall come about!',\n",
       " 'I warrant, an I should live a thousand years,',\n",
       " \"I never should forget it: 'Wilt thou not, Jule?' quoth he;\",\n",
       " \"And, pretty fool, it stinted and said 'Ay.'\",\n",
       " 'LADY CAPULET\\tEnough of this; I pray thee, hold thy peace.',\n",
       " 'Nurse\\tYes, madam: yet I cannot choose but laugh,',\n",
       " \"To think it should leave crying and say 'Ay.'\",\n",
       " 'And yet, I warrant, it had upon its brow',\n",
       " \"A bump as big as a young cockerel's stone;\",\n",
       " 'A parlous knock; and it cried bitterly:',\n",
       " \"'Yea,' quoth my husband,'fall'st upon thy face?\",\n",
       " 'Thou wilt fall backward when thou comest to age;',\n",
       " \"Wilt thou not, Jule?' it stinted and said 'Ay.'\",\n",
       " 'JULIET\\tAnd stint thou too, I pray thee, nurse, say I.',\n",
       " 'Nurse\\tPeace, I have done. God mark thee to his grace!',\n",
       " \"Thou wast the prettiest babe that e'er I nursed:\",\n",
       " 'An I might live to see thee married once,',\n",
       " 'I have my wish.',\n",
       " \"LADY CAPULET\\tMarry, that 'marry' is the very theme\",\n",
       " 'I came to talk of. Tell me, daughter Juliet,',\n",
       " 'How stands your disposition to be married?',\n",
       " 'JULIET\\tIt is an honour that I dream not of.',\n",
       " 'Nurse\\tAn honour! were not I thine only nurse,',\n",
       " \"I would say thou hadst suck'd wisdom from thy teat.\",\n",
       " 'LADY CAPULET\\tWell, think of marriage now; younger than you,',\n",
       " 'Here in Verona, ladies of esteem,',\n",
       " 'Are made already mothers: by my count,',\n",
       " 'I was your mother much upon these years',\n",
       " 'That you are now a maid. Thus then in brief:',\n",
       " 'The valiant Paris seeks you for his love.',\n",
       " 'Nurse\\tA man, young lady! lady, such a man',\n",
       " \"As all the world--why, he's a man of wax.\",\n",
       " \"LADY CAPULET\\tVerona's summer hath not such a flower.\",\n",
       " \"Nurse\\tNay, he's a flower; in faith, a very flower.\",\n",
       " 'LADY CAPULET\\tWhat say you? can you love the gentleman?',\n",
       " 'This night you shall behold him at our feast;',\n",
       " \"Read o'er the volume of young Paris' face,\",\n",
       " \"And find delight writ there with beauty's pen;\",\n",
       " 'Examine every married lineament,',\n",
       " 'And see how one another lends content',\n",
       " 'And what obscured in this fair volume lies',\n",
       " 'Find written in the margent of his eyes.',\n",
       " 'This precious book of love, this unbound lover,',\n",
       " 'To beautify him, only lacks a cover:',\n",
       " \"The fish lives in the sea, and 'tis much pride\",\n",
       " 'For fair without the fair within to hide:',\n",
       " \"That book in many's eyes doth share the glory,\",\n",
       " 'That in gold clasps locks in the golden story;',\n",
       " 'So shall you share all that he doth possess,',\n",
       " 'By having him, making yourself no less.',\n",
       " 'Nurse\\tNo less! nay, bigger; women grow by men.',\n",
       " \"LADY CAPULET\\tSpeak briefly, can you like of Paris' love?\",\n",
       " \"JULIET\\tI'll look to like, if looking liking move:\",\n",
       " 'But no more deep will I endart mine eye',\n",
       " 'Than your consent gives strength to make it fly.',\n",
       " '[Enter a Servant]',\n",
       " 'Servant\\tMadam, the guests are come, supper served up, you',\n",
       " 'called, my young lady asked for, the nurse cursed in',\n",
       " 'the pantry, and every thing in extremity. I must',\n",
       " 'hence to wait; I beseech you, follow straight.',\n",
       " 'LADY CAPULET\\tWe follow thee.',\n",
       " '[Exit Servant]',\n",
       " 'Juliet, the county stays.',\n",
       " 'Nurse\\tGo, girl, seek happy nights to happy days.',\n",
       " '[Exeunt]',\n",
       " 'ROMEO AND JULIET',\n",
       " 'ACT I',\n",
       " 'SCENE IV\\tA street.',\n",
       " '[Enter ROMEO, MERCUTIO, BENVOLIO, with five or six',\n",
       " 'Maskers, Torch-bearers, and others]',\n",
       " 'ROMEO\\tWhat, shall this speech be spoke for our excuse?',\n",
       " 'Or shall we on without a apology?',\n",
       " 'BENVOLIO\\tThe date is out of such prolixity:',\n",
       " \"We'll have no Cupid hoodwink'd with a scarf,\",\n",
       " \"Bearing a Tartar's painted bow of lath,\",\n",
       " 'Scaring the ladies like a crow-keeper;',\n",
       " 'Nor no without-book prologue, faintly spoke',\n",
       " 'After the prompter, for our entrance:',\n",
       " 'But let them measure us by what they will;',\n",
       " \"We'll measure them a measure, and be gone.\",\n",
       " 'ROMEO\\tGive me a torch: I am not for this ambling;',\n",
       " 'Being but heavy, I will bear the light.',\n",
       " 'MERCUTIO\\tNay, gentle Romeo, we must have you dance.',\n",
       " 'ROMEO\\tNot I, believe me: you have dancing shoes',\n",
       " 'With nimble soles: I have a soul of lead',\n",
       " 'So stakes me to the ground I cannot move.',\n",
       " \"MERCUTIO\\tYou are a lover; borrow Cupid's wings,\",\n",
       " 'And soar with them above a common bound.',\n",
       " 'ROMEO\\tI am too sore enpierced with his shaft',\n",
       " 'To soar with his light feathers, and so bound,',\n",
       " 'I cannot bound a pitch above dull woe:',\n",
       " \"Under love's heavy burden do I sink.\",\n",
       " 'MERCUTIO\\tAnd, to sink in it, should you burden love;',\n",
       " 'Too great oppression for a tender thing.',\n",
       " 'ROMEO\\tIs love a tender thing? it is too rough,',\n",
       " 'Too rude, too boisterous, and it pricks like thorn.',\n",
       " 'MERCUTIO\\tIf love be rough with you, be rough with love;',\n",
       " 'Prick love for pricking, and you beat love down.',\n",
       " 'Give me a case to put my visage in:',\n",
       " 'A visor for a visor! what care I',\n",
       " 'What curious eye doth quote deformities?',\n",
       " 'Here are the beetle brows shall blush for me.',\n",
       " 'BENVOLIO\\tCome, knock and enter; and no sooner in,',\n",
       " 'But every man betake him to his legs.',\n",
       " 'ROMEO\\tA torch for me: let wantons light of heart',\n",
       " 'Tickle the senseless rushes with their heels,',\n",
       " \"For I am proverb'd with a grandsire phrase;\",\n",
       " \"I'll be a candle-holder, and look on.\",\n",
       " \"The game was ne'er so fair, and I am done.\",\n",
       " \"MERCUTIO\\tTut, dun's the mouse, the constable's own word:\",\n",
       " \"If thou art dun, we'll draw thee from the mire\",\n",
       " \"Of this sir-reverence love, wherein thou stick'st\",\n",
       " 'Up to the ears. Come, we burn daylight, ho!',\n",
       " \"ROMEO\\tNay, that's not so.\",\n",
       " 'MERCUTIO\\tI mean, sir, in delay',\n",
       " 'We waste our lights in vain, like lamps by day.',\n",
       " 'Take our good meaning, for our judgment sits',\n",
       " 'Five times in that ere once in our five wits.',\n",
       " 'ROMEO\\tAnd we mean well in going to this mask;',\n",
       " \"But 'tis no wit to go.\",\n",
       " 'MERCUTIO\\tWhy, may one ask?',\n",
       " \"ROMEO\\tI dream'd a dream to-night.\",\n",
       " 'MERCUTIO\\tAnd so did I.',\n",
       " 'ROMEO\\tWell, what was yours?',\n",
       " 'MERCUTIO\\tThat dreamers often lie.',\n",
       " 'ROMEO\\tIn bed asleep, while they do dream things true.',\n",
       " 'MERCUTIO\\tO, then, I see Queen Mab hath been with you.',\n",
       " \"She is the fairies' midwife, and she comes\",\n",
       " 'In shape no bigger than an agate-stone',\n",
       " 'On the fore-finger of an alderman,',\n",
       " 'Drawn with a team of little atomies',\n",
       " \"Athwart men's noses as they lie asleep;\",\n",
       " \"Her wagon-spokes made of long spiders' legs,\",\n",
       " 'The cover of the wings of grasshoppers,',\n",
       " \"The traces of the smallest spider's web,\",\n",
       " \"The collars of the moonshine's watery beams,\",\n",
       " \"Her whip of cricket's bone, the lash of film,\",\n",
       " 'Her wagoner a small grey-coated gnat,',\n",
       " 'Not so big as a round little worm',\n",
       " \"Prick'd from the lazy finger of a maid;\",\n",
       " 'Her chariot is an empty hazel-nut',\n",
       " 'Made by the joiner squirrel or old grub,',\n",
       " \"Time out o' mind the fairies' coachmakers.\",\n",
       " 'And in this state she gallops night by night',\n",
       " \"Through lovers' brains, and then they dream of love;\",\n",
       " \"O'er courtiers' knees, that dream on court'sies straight,\",\n",
       " \"O'er lawyers' fingers, who straight dream on fees,\",\n",
       " \"O'er ladies ' lips, who straight on kisses dream,\",\n",
       " 'Which oft the angry Mab with blisters plagues,',\n",
       " 'Because their breaths with sweetmeats tainted are:',\n",
       " \"Sometime she gallops o'er a courtier's nose,\",\n",
       " 'And then dreams he of smelling out a suit;',\n",
       " \"And sometime comes she with a tithe-pig's tail\",\n",
       " \"Tickling a parson's nose as a' lies asleep,\",\n",
       " 'Then dreams, he of another benefice:',\n",
       " \"Sometime she driveth o'er a soldier's neck,\",\n",
       " 'And then dreams he of cutting foreign throats,',\n",
       " 'Of breaches, ambuscadoes, Spanish blades,',\n",
       " 'Of healths five-fathom deep; and then anon',\n",
       " 'Drums in his ear, at which he starts and wakes,',\n",
       " 'And being thus frighted swears a prayer or two',\n",
       " 'And sleeps again. This is that very Mab',\n",
       " 'That plats the manes of horses in the night,',\n",
       " 'And bakes the elflocks in foul sluttish hairs,',\n",
       " 'Which once untangled, much misfortune bodes:',\n",
       " 'This is the hag, when maids lie on their backs,',\n",
       " 'That presses them and learns them first to bear,',\n",
       " 'Making them women of good carriage:',\n",
       " 'This is she--',\n",
       " 'ROMEO\\t                  Peace, peace, Mercutio, peace!',\n",
       " \"Thou talk'st of nothing.\",\n",
       " 'MERCUTIO\\tTrue, I talk of dreams,',\n",
       " 'Which are the children of an idle brain,',\n",
       " 'Begot of nothing but vain fantasy,',\n",
       " 'Which is as thin of substance as the air',\n",
       " 'And more inconstant than the wind, who wooes',\n",
       " 'Even now the frozen bosom of the north,',\n",
       " \"And, being anger'd, puffs away from thence,\",\n",
       " 'Turning his face to the dew-dropping south.',\n",
       " 'BENVOLIO\\tThis wind, you talk of, blows us from ourselves;',\n",
       " 'Supper is done, and we shall come too late.',\n",
       " 'ROMEO\\tI fear, too early: for my mind misgives',\n",
       " 'Some consequence yet hanging in the stars',\n",
       " 'Shall bitterly begin his fearful date',\n",
       " \"With this night's revels and expire the term\",\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-zMCe7aJkGwA",
    "outputId": "c0eace05-246a-47d9-9542-f009a4940836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 A LOVER'S COMPLAINT\n",
      "Sample line at position 999 With this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6XsiyHvkGwD"
   },
   "source": [
    "Notice that the letters are both uppercase and lowercase.  In order to reduce the complexity of the task, we will convert all characters to lowercase.  This way, the model only needs to predict the likelihood that a letter is 'a' and not decide between uppercase 'A' and lowercase 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "UBO9jI8EkGwE",
    "outputId": "55b55d61-a5b1-4381-ff88-d146071ac671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 a lover's complaint\n",
      "Sample line at position 999 with this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "# go through each line\n",
    "for i, line in enumerate(lines):\n",
    "    # convert to all lowercase\n",
    "    lines[i] = line.lower()\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "lines = lines[:-1000] # Leave the rest for training\n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDcxEmX31y3d"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Convert a line to tensor\n",
    "\n",
    "Now that you have your list of lines, you will convert each character in that list to a number. You can use Python's `ord` function to do it. \n",
    "\n",
    "Given a string representing of one Unicode character, the `ord` function return an integer representing the Unicode code point of that character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Cc_B8ae3kGwI",
    "outputId": "94eb6798-827a-4494-dec0-7bb84523ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord('a'): 97\n",
      "ord('b'): 98\n",
      "ord('c'): 99\n",
      "ord(' '): 32\n",
      "ord('x'): 120\n",
      "ord('y'): 121\n",
      "ord('z'): 122\n",
      "ord('1'): 49\n",
      "ord('2'): 50\n",
      "ord('125'): 53\n"
     ]
    }
   ],
   "source": [
    "# View the unique unicode integer associated with each character\n",
    "print(f\"ord('a'): {ord('a')}\")\n",
    "print(f\"ord('b'): {ord('b')}\")\n",
    "print(f\"ord('c'): {ord('c')}\")\n",
    "print(f\"ord(' '): {ord(' ')}\")\n",
    "print(f\"ord('x'): {ord('x')}\")\n",
    "print(f\"ord('y'): {ord('y')}\")\n",
    "print(f\"ord('z'): {ord('z')}\")\n",
    "print(f\"ord('1'): {ord('1')}\")\n",
    "print(f\"ord('2'): {ord('2')}\")\n",
    "print(f\"ord('125'): {ord('5')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "a='abc'\n",
    "for c in a:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWB9qOLOkGwL"
   },
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Write a function that takes in a single line and transforms each character into its unicode integer.  This returns a list of integers, which we'll refer to as a tensor.\n",
    "- Use a special integer to represent the end of the sentence (the end of the line).\n",
    "- This will be the EOS_int (end of sentence integer) parameter of the function.\n",
    "- Include the EOS_int as the last integer of the \n",
    "- For this exercise, you will use the number `1` to represent the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IO4NSPkOITNK"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: line_to_tensor\n",
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \"\"\"Turns a line of text into a tensor\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of integers (unicode values) for the characters in the `line`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the tensor as an empty list\n",
    "    tensor = []\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        \n",
    "        # convert to unicode int\n",
    "        c_int = ord(c)\n",
    "        \n",
    "        # append the unicode integer to the tensor list\n",
    "        tensor.append(c_int)\n",
    "    \n",
    "    # include the end-of-sentence integer\n",
    "    tensor.append(EOS_int)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D9Z_vtI7tTcw",
    "outputId": "0423ad21-af3e-4e6d-a558-472f4bf5f964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your output\n",
    "type(line_to_tensor('abc xyz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MwEspKCtTc4"
   },
   "source": [
    "##### Expected Output\n",
    "```CPP\n",
    "[97, 98, 99, 32, 120, 121, 122, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_line_to_tensor(line_to_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFOR19cX2TQs"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 Batch generator \n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. Here, you will build a data generator that takes in a text and returns a batch of text lines (lines are sentences).\n",
    "- The generator converts text lines (sentences) into numpy arrays of integers padded by zeros so that all arrays have the same length, which is the length of the longest sentence in the entire data set.\n",
    "\n",
    "Once you create the generator, you can iterate on it like this:\n",
    "\n",
    "```\n",
    "next(data_generator)\n",
    "```\n",
    "\n",
    "This generator returns the data in a format that you could directly use in your model when computing the feed-forward of your algorithm. This iterator returns a batch of lines and per token mask. The batch is a tuple of three parts: inputs, targets, mask. The inputs and targets are identical. The second column will be used to evaluate your predictions. Mask is 1 for non-padding tokens.\n",
    "\n",
    "<a name='ex02'></a>\n",
    "### Exercise 02\n",
    "**Instructions:** Implement the data generator below. Here are some things you will need. \n",
    "\n",
    "- While True loop: this will yield one batch at a time.\n",
    "- if index >= num_lines, set index to 0. \n",
    "- The generator should return shuffled batches of data. To achieve this without modifying the actual lines a list containing the indexes of `data_lines` is created. This list can be shuffled and used to get random batches everytime the index is reset.\n",
    "- if len(line) < max_length append line to cur_batch.\n",
    "    - Note that a line that has length equal to max_length should not be appended to the batch. \n",
    "    - This is because when converting the characters into a tensor of integers, an additional end of sentence token id will be added.  \n",
    "    - So if max_length is 5, and a line has 4 characters, the tensor representing those 4 characters plus the end of sentence character will be of length 5, which is the max length.\n",
    "- if len(cur_batch) == batch_size, go over every line, convert it to an int and store it.\n",
    "\n",
    "**Remember that when calling np you are really calling trax.fastmath.numpy which is trax’s version of numpy that is compatible with JAX. As a result of this, where you used to encounter the type numpy.ndarray now you will find the type jax.interpreters.xla.DeviceArray.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ekSEQlvtTc7"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the line_to_tensor function above inside a list comprehension in order to pad lines with zeros.</li>\n",
    "    <li>Keep in mind that the length of the tensor is always 1 + the length of the original line of characters.  Keep this in mind when setting the padding of zeros.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMingz5xzrGD"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added\n",
    "                to the tensor.  \n",
    "                Keep in mind that the length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        data_lines (list): list of the sentences to group into batches.\n",
    "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
    "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
    "    \"\"\"\n",
    "    # initialize the index that points to the current position in the lines index array\n",
    "    index = 0\n",
    "    \n",
    "    # initialize the list that will contain the current batch\n",
    "    cur_batch = []\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(data_lines)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle line indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    while True:\n",
    "        \n",
    "        # if the index is greater than or equal to the number of lines in data_lines\n",
    "        if index>=num_lines:\n",
    "            # then reset the index to 0\n",
    "            index = 0\n",
    "            # shuffle line indexes if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index) \n",
    "                            \n",
    "        # get a line at the `lines_index[index]` position in data_lines\n",
    "        line = data_lines[lines_index[index]]\n",
    "        \n",
    "        # if the length of the line is less than max_length\n",
    "        if len(line)<max_length:\n",
    "            # append the line to the current batch\n",
    "            cur_batch.append(line)\n",
    "            \n",
    "        # increment the index by one\n",
    "        index += 1\n",
    "        \n",
    "        # if the current batch is now equal to the desired batch size\n",
    "        if len(cur_batch)==batch_size:\n",
    "            \n",
    "            batch = []\n",
    "            mask = []\n",
    "            \n",
    "            # go through each line (li) in cur_batch\n",
    "            for li in cur_batch:\n",
    "                # convert the line (li) to a tensor of integers\n",
    "                tensor =line_to_tensor(li, EOS_int=1) \n",
    "                \n",
    "                # Create a list of zeros to represent the padding\n",
    "                # so that the tensor plus padding will have length `max_length`\n",
    "                pad = [0] * (max_length-len(tensor))\n",
    "                \n",
    "                # combine the tensor plus pad\n",
    "                tensor_pad = tensor+pad\n",
    "                \n",
    "                # append the padded tensor to the batch\n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                # A mask for this tensor_pad is 1 whereever tensor_pad is not\n",
    "                # 0 and 0 whereever tensor_pad is 0, i.e. if tensor_pad is\n",
    "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
    "                # [1, 1, 1, 0, 0, 0]\n",
    "                example_mask =[1 if item>0 else 0 for item in tensor_pad] \n",
    "                mask.append(example_mask) # @ KEEPTHIS\n",
    "               \n",
    "            # convert the batch (data type list) to a numpy array\n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "            \n",
    "            ### END CODE HERE ##\n",
    "            \n",
    "            # Yield two copies of the batch and mask.\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "            \n",
    "            # reset the current batch to an empty list\n",
    "            cur_batch = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out your data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '234567890', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = data_generator(batch_size=2, \n",
    "                              max_length=10, \n",
    "                              data_lines=tmp_lines,\n",
    "                              shuffle=False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rWMOk7ikGwZ"
   },
   "source": [
    "##### Expected output\n",
    "\n",
    "```CPP\n",
    "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
    "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
    " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
    "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
    " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_data_generator(data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-M0U9GDwt0r"
   },
   "source": [
    "Now that you have your generator, you can just call them and they will return tensors which correspond to your lines in Shakespeare. The first column and the second column are identical. Now you can go ahead and start building your neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFcB-i-rDd68"
   },
   "source": [
    "<a name='1.4'></a>\n",
    "### 1.4 Repeating Batch generator \n",
    "\n",
    "The way the iterator is currently defined, it will keep providing batches forever.\n",
    "\n",
    "Although it is not needed, we want to show you the `itertools.cycle` function which is really useful when the generator eventually stops\n",
    "\n",
    "Notice that it is expected to use this function within the training function further below\n",
    "\n",
    "Usually we want to cycle over the dataset multiple times during training (i.e. train for multiple *epochs*).\n",
    "\n",
    "For small datasets we can use [`itertools.cycle`](https://docs.python.org/3.8/library/itertools.html#itertools.cycle) to achieve this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v589leeZETy7"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "infinite_data_generator = itertools.cycle(\n",
    "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWvsSxDUFB0p"
   },
   "source": [
    "You can see that we can get more than the 5 lines in tmp_lines using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0lJhBPgJFAxb",
    "outputId": "0849fa48-0d82-4050-b3b4-e738d96a7ca8"
   },
   "outputs": [],
   "source": [
    "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
    "print(len(ten_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmZRBoaMwt0w"
   },
   "source": [
    "<a name='2'></a>\n",
    "\n",
    "# Part 2: Defining the GRU model\n",
    "\n",
    "Now that you have the input and output tensors, you will go ahead and initialize your model. You will be implementing the `GRULM`, gated recurrent unit model. To implement this model, you will be using google's `trax` package. Instead of making you implement the `GRU` from scratch, we will give you the necessary methods from a build in package. You can use the following packages when constructing the model: \n",
    "\n",
    "\n",
    "- `tl.Serial`: Combinator that applies layers serially (by function composition). [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/combinators.py#L26)\n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`\n",
    "\n",
    "___\n",
    "\n",
    "- `tl.ShiftRight`: Allows the model to go right in the feed forward. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/attention.py#L560)\n",
    "    - `ShiftRight(n_shifts=1, mode='train')` layer to shift the tensor to the right n_shift times\n",
    "    - Here in the exercise you only need to specify the mode and not worry about n_shifts\n",
    "\n",
    "___\n",
    "\n",
    "- `tl.Embedding`: Initializes the embedding. In this case it is the size of the vocabulary by the dimension of the model. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L130) \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "___\n",
    "\n",
    "- `tl.GRU`: `Trax` GRU layer. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRU) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/rnn.py#L154)\n",
    "    - `GRU(n_units)` Builds a traditional GRU of n_cells with dense internal transformations.\n",
    "    - `GRU` paper: https://arxiv.org/abs/1412.3555\n",
    "___\n",
    "\n",
    "- `tl.Dense`: A dense layer. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L34)\n",
    "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.\n",
    "___\n",
    "\n",
    "- `tl.LogSoftmax`: Log of the output probabilities. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) / [source code](https://github.com/google/trax/blob/e65d51fe584b10c0fa0fccadc1e70b6330aac67e/trax/layers/core.py#L644)\n",
    "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
    "___\n",
    "\n",
    "<a name='ex03'></a>\n",
    "### Exercise 03\n",
    "**Instructions:** Implement the `GRULM` class below. You should be using all the methods explained above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hww76f8_wt0x"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: GRULM\n",
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    \"\"\"Returns a GRU language model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
    "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    model = tl.Serial( \n",
    "      tl.ShiftRight(1,mode='train'), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size,d_model), # Stack the embedding layer\n",
    "    #[tl.GRU(n_units=model_dimension) for _ in range(n_layers)]\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(vocab_size), # Dense layer\n",
    "      tl.LogSoftmax(), # Log Softmax\n",
    "    ) \n",
    "    ### END CODE HERE ###\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "kvQ_jf52-JAn",
    "outputId": "9d13c577-f89d-427a-8944-a00ca57a4f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# testing your model\n",
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3K8UKp48kGwi"
   },
   "source": [
    "##### Expected output\n",
    "\n",
    "```python\n",
    "Serial[\n",
    "  Serial[\n",
    "    ShiftRight(1)\n",
    "  ]\n",
    "  Embedding_256_512\n",
    "  GRU_512\n",
    "  GRU_512\n",
    "  Dense_256\n",
    "  LogSoftmax\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_GRULM(GRULM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsvjaCQ6wt02"
   },
   "source": [
    "<a name='3'></a>\n",
    "# Part 3: Training\n",
    "\n",
    "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. You also have to feed in a built model. Before, going into the training, we re-introduce the `TrainTask` and `EvalTask` abstractions from the last week's assignment.\n",
    "\n",
    "To train a model on a task, Trax defines an abstraction `trax.supervised.training.TrainTask` which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly to evaluate a model, Trax defines an abstraction `trax.supervised.training.EvalTask` which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the `trax.supervised.training.Loop` abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "Using `training.Loop` will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Birerv82mLv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxRzCDPJFGxo"
   },
   "source": [
    "An `epoch` is traditionally defined as one pass through the dataset.\n",
    "\n",
    "Since the dataset was divided in `batches` you need several `steps` (gradient evaluations) in order to complete an `epoch`. So, one `epoch` corresponds to the number of examples in a `batch` times the number of `steps`. In short, in each `epoch` you go over all the dataset. \n",
    "\n",
    "The `max_length` variable defines the maximum length of lines to be used in training our data, lines longer than that length are discarded. \n",
    "\n",
    "Below is a function and results that indicate how many lines conform to our criteria of maximum length of a sentence in the entire dataset and how many `steps` are required in order to cover the entire dataset which in turn corresponds to an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "T3NxHd-VtTcb",
    "outputId": "b945b175-3101-4600-ac2f-8c705c13d752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 25881\n",
      "Batch size (a power of 2): 32\n",
      "Number of steps to cover one epoch: 808\n"
     ]
    }
   ],
   "source": [
    "def n_used_lines(lines, max_length):\n",
    "    '''\n",
    "    Args: \n",
    "    lines: all lines of text an array of lines\n",
    "    max_length - max_length of a line in order to be considered an int\n",
    "    output_dir - folder to save your file an int\n",
    "    Return:\n",
    "    number of efective examples\n",
    "    '''\n",
    "\n",
    "    n_lines = 0\n",
    "    for l in lines:\n",
    "        if len(l) <= max_length:\n",
    "            n_lines += 1\n",
    "    return n_lines\n",
    "\n",
    "num_used_lines = n_used_lines(lines, 32)\n",
    "print('Number of used lines from the dataset:', num_used_lines)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_used_lines/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwUM5RwHHYP8"
   },
   "source": [
    "**Expected output:** \n",
    "\n",
    "Number of used lines from the dataset: 25881\n",
    "\n",
    "Batch size (a power of 2): 32\n",
    "\n",
    "Number of steps to cover one epoch: 808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgFMfH5awt07"
   },
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. \n",
    "\n",
    "<a name='ex04'></a>\n",
    "### Exercise 04\n",
    "\n",
    "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n",
    "\n",
    "- Create a `trax.supervised.trainer.TrainTask` object, this encapsulates the aspects of the dataset and the problem at hand:\n",
    "    - labeled_data = the labeled data that we want to *train* on.\n",
    "    - loss_fn = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=CrossEntropyLoss#trax.layers.metrics.CrossEntropyLoss)\n",
    "    - optimizer = [trax.optimizers.Adam()](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=Adam#trax.optimizers.adam.Adam) with learning rate = 0.0005\n",
    "\n",
    "- Create a `trax.supervised.trainer.EvalTask` object, this encapsulates aspects of evaluating the model:\n",
    "    - labeled_data = the labeled data that we want to *evaluate* on.\n",
    "    - metrics = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss) and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n",
    "    - How frequently we want to evaluate and checkpoint the model.\n",
    "\n",
    "- Create a `trax.supervised.trainer.Loop` object, this encapsulates the following:\n",
    "    - The previously created `TrainTask` and `EvalTask` objects.\n",
    "    - the training model = [GRULM](#ex03)\n",
    "    - optionally the evaluation model, if different from the training model. NOTE: in presence of Dropout etc we usually want the evaluation model to behave slightly differently than the training model.\n",
    "\n",
    "You will be using a cross entropy loss, with Adam optimizer. Please read the [trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. Make sure you use the number of steps provided as a parameter to train for the desired number of steps.\n",
    "\n",
    "**NOTE:** Don't forget to wrap the data generator in `itertools.cycle` to iterate on it for multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kbtfz4T_m7x"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(model, data_generator, lines, eval_lines, batch_size=32, max_length=64, n_steps=1, output_dir='model/'): \n",
    "    \"\"\"Function that trains the model\n",
    "\n",
    "    Args:\n",
    "        model (trax.layers.combinators.Serial): GRU model.\n",
    "        data_generator (function): Data generator function.\n",
    "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
    "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
    "        lines (list): List of lines to use for training. Defaults to lines.\n",
    "        eval_lines (list): List of lines to use for evaluation. Defaults to eval_lines.\n",
    "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
    "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    bare_train_generator = data_generator(batch_size, max_length, lines, line_to_tensor=line_to_tensor, shuffle=True)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator =data_generator(batch_size, max_length, eval_lines, line_to_tensor=line_to_tensor, shuffle=True)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "    \n",
    "    train_task = training.TrainTask( \n",
    "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
    "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
    "        optimizer=trax.optimizers.Adam(learning_rate = 0.0005)     # Don't forget to add the learning rate parameter TO 0.0005\n",
    "    ) \n",
    "    \n",
    "    eval_task = training.EvalTask( \n",
    "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
    "        metrics=[tl.CrossEntropyLoss(),tl.Accuracy()], # Don't forget to instantiate these objects\n",
    "        n_eval_batches=3  # For better evaluation accuracy in reasonable time \n",
    "    ) \n",
    "    \n",
    "    training_loop = training.Loop(model, \n",
    "                                  train_task, \n",
    "                                  eval_tasks=[eval_task], \n",
    "                                  output_dir=output_dir) \n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # We return this because it contains a handle to the model, which has the weights etc.\n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.cycle at 0x7fc48c1964b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "itertools.cycle('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SwP646GpK3pD",
    "outputId": "4c88bcf5-a8aa-4160-cc3c-3ff16f39fd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 3411200\n",
      "Step      1: Ran 1 train steps in 6.94 secs\n",
      "Step      1: train CrossEntropyLoss |  5.54520130\n",
      "Step      1: eval  CrossEntropyLoss |  5.54095840\n",
      "Step      1: eval          Accuracy |  0.16060416\n"
     ]
    }
   ],
   "source": [
    "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
    "output_dir = './model/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_dir)\n",
    "except OSError as e:\n",
    "    pass\n",
    "\n",
    "training_loop = train_model(GRULM(), data_generator, lines=lines, eval_lines=eval_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was only trained for 1 step due to the constraints of this environment. Even on a GPU accelerated environment it will take many hours for it to achieve a good level of accuracy. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function. This cell may take some seconds to execute.\n",
    "w2_unittest.test_train_model(train_model, GRULM(), data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abKPe7d4wt1C"
   },
   "source": [
    "<a name='4'></a>\n",
    "# Part 4:  Evaluation  \n",
    "<a name='4.1'></a>\n",
    "### 4.1 Evaluating using the deep nets\n",
    "\n",
    "Now that you have learned how to train a model, you will learn how to evaluate it. To evaluate language models, we usually use perplexity which is a measure of how well a probability model predicts a sample. Note that perplexity is defined as: \n",
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good).\n",
    "\n",
    "\n",
    "$$\\log P(W) = {\\log\\left(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\right)}$$$$ = \\log\\left(\\left(\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}\\right)^{\\frac{1}{N}}\\right)$$\n",
    "$$ = \\log\\left(\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\right)^{-\\frac{1}{N}}\\right)$$$$ = -\\frac{1}{N}{\\log\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\right)} $$$$ = -\\frac{1}{N}{{\\sum_{i=1}^{N}{\\log P(w_i| w_1,...,w_{n-1})}}} $$\n",
    "\n",
    "<a name='ex05'></a>\n",
    "### Exercise 05\n",
    "**Instructions:** Write a program that will help evaluate your model. Implementation hack: your program takes in preds and target. Preds is a tensor of log probabilities. You can use [`tl.one_hot`](https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L154) to transform the target into the same dimension. You then multiply them and sum. \n",
    "\n",
    "You also have to create a mask to only get the non-padded probabilities. Good luck! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PqIvySI6ZWb"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>To convert the target into the same dimension as the predictions tensor use tl.one.hot with target and preds.shape[-1].</li>\n",
    "    <li>You will also need the np.equal function in order to unpad the data and properly compute perplexity.</li>\n",
    "    <li>Keep in mind while implementing the formula above that <em> w<sub>i</sub></em> represents a letter from our 256 letter alphabet.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OtmlEuOwt1D"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(preds, target):\n",
    "    \"\"\"Function to test the model.\n",
    "\n",
    "    Args:\n",
    "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: log_perplexity of the model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "   \n",
    "    log_p = np.sum( preds* tl.one_hot(target,preds.shape[-1]), axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
    "    log_p = log_p * non_pad                           # Get rid of the padding    \n",
    "    \n",
    "    log_ppx = np.sum(log_p,axis=1) / np.sum(non_pad,axis=1) # Remember to set the axis properly when summing up\n",
    "    log_ppx = np.mean(log_ppx) # Compute the mean of the previous expression\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0]\n",
      " [0 5 6]\n",
      " [1 2 3]] [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]] [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] [[1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]] [[1. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[1,2,0],[0,5,6],[1,2,3]])\n",
    "B=tl.one_hot(A,12)\n",
    "C=np.sum(B*B,axis=-1)\n",
    "n=1-np.equal(A,0)\n",
    "nn=n*C\n",
    "print(A,B,C,n,nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xl8X0FPAwt1F",
    "outputId": "1dbfef92-c8ca-4cae-c92c-7b1b4adb6963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 1.7646704 5.8396473\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Testing \n",
    "model = GRULM()\n",
    "model.init_from_file('model.pkl.gz')\n",
    "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
    "preds = model(batch[0])\n",
    "log_ppx = test_model(preds, batch[1])\n",
    "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PZdy1V2wt1H"
   },
   "source": [
    "**Expected Output:** The log perplexity and perplexity of your model are respectively around 1.7 and 5.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "pretrained_model = GRULM()\n",
    "pretrained_model.init_from_file('model.pkl.gz')\n",
    "w2_unittest.unittest_test_model(test_model, pretrained_model)\n",
    "del pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-STC44Ywt1I"
   },
   "source": [
    "<a name='5'></a>\n",
    "# Part 5: Generating the language with your own model\n",
    "\n",
    "We will now use your own language model to generate new sentences for that we need to make draws from a Gumble distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXdIBCGxtTdt"
   },
   "source": [
    "The Gumbel Probability Density Function (PDF) is defined as: \n",
    "\n",
    "$$ f(z) = {1\\over{\\beta}}e^{(-z+e^{(-z)})} $$\n",
    "\n",
    "where: $$ z = {(x - \\mu)\\over{\\beta}}$$\n",
    "\n",
    "The maximum value, which is what we choose as the prediction in the last step of a Recursive Neural Network `RNN` we are using for text generation, in a sample of a random variable following an exponential distribution approaches the Gumbel distribution when the sample increases asymptotically. For that reason, the Gumbel distribution is used to sample from a categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xrOJHbXewt1J",
    "outputId": "665bc6ff-f9ee-4097-c89b-83ef0e12c1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of himself stay, good!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate some news sentence\n",
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heaty fith'd provicad'sly shiff \n",
      "go door of my hones, so stand is\n",
      "and throw to gowl. made you, sad\n"
     ]
    }
   ],
   "source": [
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAfV3l5Zwt1L"
   },
   "source": [
    "In the generated text above, you can see that the model generates text that makes sense capturing dependencies between words and without any input. A simple n-gram model would have not been able to capture all of that in one sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsE8tdTLwt1M"
   },
   "source": [
    "<a name='6'></a>\n",
    "###  <span style=\"color:blue\"> On statistical methods </span>\n",
    "\n",
    "Using a statistical method like the one you implemented in course 2 will not give you results that are as good. Your model will not be able to encode information seen previously in the data set and as a result, the perplexity will increase. Remember from course 2 that the higher the perplexity, the worse your model is. Furthermore, statistical ngram models take up too much space and memory. As a result, it will be inefficient and too slow. Conversely, with deepnets, you can get a better perplexity. Note, learning about n-gram language models is still important and allows you to better understand deepnets.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
